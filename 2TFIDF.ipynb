{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "data = pd.read_csv('train_2kmZucJ.csv')\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coverting Emojis to words\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text=text.replace(emot,\"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "#Coverting Emoticons to words   \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the above functions\n",
    "data['tweet']=data['tweet'].apply(convert_emojis)\n",
    "data['tweet']=data['tweet'].apply(convert_emoticons)\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing url\n",
    "def remove_url(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling url function\n",
    "data['tweet']=data['tweet'].apply(remove_url)\n",
    "data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "def tokenizer(text):\n",
    "    tk = WhitespaceTokenizer() \n",
    "    gfg = text\n",
    "    text = ' '.join(tk.tokenize(gfg))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function\n",
    "data['tweet']=data['tweet'].apply(tokenizer)\n",
    "for col in data['tweet']:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing negations by manual replacements\n",
    "j=0\n",
    "for i in data['tweet']:\n",
    "    tweet=i\n",
    "    CONTRACTIONS = {\"mayn't\":\"may not\", \"may've\":\"may have\", \"isn't\":\"is not\", \"they're\":\"they are\", \"wasn't\":\"was not\", \"weren't\":\"were not\", \"doesn't\":\"does not\", \"don't\":\"do not\", \"didn't\":\"did not\", \"hasn't\":\"has not\", \"we've\":\"we have\", \"hadn't\":\"had not\", \"can't\":\"can not\", \"couldn't\":\"could not\", \"shouldn't\":\"should not\", \"mustn't\":\"must not\",\"won't\":\"wont\" , \"might've\":\"might have\"}\n",
    "    tweet = tweet.replace(\"â€™\",\"'\")\n",
    "    words = tweet.split()\n",
    "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "    data['tweet'][j] = \" \".join(reformed)\n",
    "    j=j+1\n",
    "for col in data['tweet']:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove twitter handles (@user)\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function\n",
    "data['tweet'] = np.vectorize(remove_pattern)(data['tweet'], \"@[\\w]*\")\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations\n",
    "data['tweet'] = data['tweet'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing meaningless words\n",
    "# words = set(nltk.corpus.words.words())\n",
    "# j=0\n",
    "# for i in data['tweet']:\n",
    "#     sent=i\n",
    "#     data['tweet'][j] = \" \".join(w for w in nltk.wordpunct_tokenize(sent) if w.lower() in words or not w.isalpha())\n",
    "#     j=j+1\n",
    "# for col in data['tweet']:\n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to lowercase\n",
    "j=0\n",
    "for i in data['tweet']:\n",
    "    data['tweet'][j]=data['tweet'][j].lower()\n",
    "    j=j+1\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Replace elongated words\n",
    "# def replaceElongated(word):\n",
    "#     \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon \"\"\"\n",
    "\n",
    "#     repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "#     repl = r'\\1\\2\\3'\n",
    "#     if wordnet.synsets(word):\n",
    "#         return word\n",
    "#     repl_word = repeat_regexp.sub(repl, word)\n",
    "#     if repl_word != word:      \n",
    "#         return replaceElongated(repl_word)\n",
    "#     else:       \n",
    "#         return repl_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['tweet']=data['tweet'].apply(replaceElongated)\n",
    "# for col in data['tweet']: \n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply function on review column\n",
    "data['tweet']=data['tweet'].apply(remove_stopwords)\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function\n",
    "data['tweet']=data['tweet'].apply(denoise_text)\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Spelling correction\n",
    "# from textblob import TextBlob\n",
    "# data['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "# from autocorrect import spell\n",
    "# j=0\n",
    "# for i in data['tweet']:\n",
    "#     text=i\n",
    "#     spells = [spell(w) for w in (nltk.word_tokenize(text))]\n",
    "#     data['tweet'][j] = \" \".join(spells)\n",
    "#     j=j+1\n",
    "# for col in data['tweet']: \n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmat(text):\n",
    "    stopword = stopwords.words('english')\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    text =' '.join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])\n",
    "    return text\n",
    "\n",
    "#Stemming\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text=' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the functions\n",
    "# data['tweet'] = data['tweet'].apply(lemmat)\n",
    "# data['tweet'] = data['tweet'].apply(simple_stemmer)\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spelling correction\n",
    "from textblob import TextBlob\n",
    "data['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Short Words\n",
    "data['tweet'] = data['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into train and test\n",
    "X_train = data['tweet'].loc[:6000].values\n",
    "y_train = data['label'].loc[:6000].values\n",
    "X_test = data['tweet'].loc[6000:].values\n",
    "y_test = data['label'].loc[6000:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)\n",
    "print(train_vectors.shape, test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics  import accuracy_score\n",
    "predicted = clf.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGDClassifier model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf1 = SGDClassifier().fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted1 = clf1.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf2 = LogisticRegression().fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted2 = clf2.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf3 = DecisionTreeClassifier().fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted3 = clf3.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomforest Classifier model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf4 = DecisionTreeClassifier().fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted4 = clf4.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support vector machine\n",
    "from sklearn import svm\n",
    "clf5=svm.SVC().fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted5 = clf5.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=10)\n",
    "clf6=neigh.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted6 = clf6.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing test dataset for prediction\n",
    "data = pd.read_csv('test_oJQbWVk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing all the steps same as in training\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text=text.replace(emot,\"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "    \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet']=data['tweet'].apply(convert_emojis)\n",
    "data['tweet']=data['tweet'].apply(convert_emoticons)\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing url\n",
    "def remove_url(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet']=data['tweet'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "\n",
    "def tokenizer(text):\n",
    "    tk = WhitespaceTokenizer() \n",
    "    gfg = text\n",
    "    text = ' '.join(tk.tokenize(gfg))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet']=data['tweet'].apply(tokenizer)\n",
    "for col in data['tweet']:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing negations\n",
    "j=0\n",
    "for i in data['tweet']:\n",
    "    tweet=i\n",
    "    CONTRACTIONS = {\"mayn't\":\"may not\", \"may've\":\"may have\", \"isn't\":\"is not\", \"they're\":\"they are\", \"wasn't\":\"was not\", \"weren't\":\"were not\", \"doesn't\":\"does not\", \"don't\":\"do not\", \"didn't\":\"did not\", \"hasn't\":\"has not\", \"we've\":\"we have\", \"hadn't\":\"had not\", \"can't\":\"can not\", \"couldn't\":\"could not\", \"shouldn't\":\"should not\", \"mustn't\":\"must not\",\"won't\":\"wont\" , \"might've\":\"might have\"}\n",
    "    tweet = tweet.replace(\"â€™\",\"'\")\n",
    "    words = tweet.split()\n",
    "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "    data['tweet'][j] = \" \".join(reformed)\n",
    "    j=j+1\n",
    "for col in data['tweet']:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove twitter handles (@user)\n",
    "data['tweet'] = np.vectorize(remove_pattern)(data['tweet'], \"@[\\w]*\")\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations\n",
    "data['tweet'] = data['tweet'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to lowercase\n",
    "j=0\n",
    "for i in data['tweet']:\n",
    "    data['tweet'][j]=data['tweet'][j].lower()\n",
    "    j=j+1\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply function on review column\n",
    "data['tweet']=data['tweet'].apply(remove_stopwords)\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet']=data['tweet'].apply(denoise_text)\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmat(text):\n",
    "    stopword = stopwords.words('english')\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    text =' '.join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])\n",
    "    return text\n",
    "\n",
    "#Stemming\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text=' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['tweet'] = data['tweet'].apply(lemmat)\n",
    "# data['tweet'] = data['tweet'].apply(simple_stemmer)\n",
    "# for col in data['tweet']: \n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spelling correction\n",
    "from textblob import TextBlob\n",
    "data['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Short Words\n",
    "data['tweet'] = data['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\n",
    "for col in data['tweet']: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data['tweet'].values\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming into TFIDF vectors\n",
    "test_vectors = vectorizer.transform(X_test)\n",
    "test_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "y_pred = clf1.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicted results\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Dataframe\n",
    "y=pd.DataFrame(data=data['id'],columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['label']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting the dataframe to csv\n",
    "y.to_csv(\"result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the csv file\n",
    "a=pd.read_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
